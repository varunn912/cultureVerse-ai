{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14192765,"sourceType":"datasetVersion","datasetId":9050015}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =================================================================================\n# üíé CULTUREVERSE FINAL: ROBUST KAGGLE EDITION\n# =================================================================================\n# ‚úÖ 1 Epoch = 781 Steps (Optimized for 0.16 Loss)\n# ‚úÖ Strict \"6-Heading\" Output Format\n# ‚úÖ Robust Hybrid System for New Idioms\n# =================================================================================\n\nimport os\nimport sys\nimport warnings\nimport gc\nimport shutil\nimport pickle\nimport numpy as np\n\n# --- 1. AGGRESSIVE DEPENDENCY FIX (CRITICAL - DO NOT TOUCH) ---\nprint(\"üõ†Ô∏è CLEANING ENVIRONMENT & INSTALLING STACK...\")\n\n# 1. Force uninstall conflicting libraries\nos.system(\"pip uninstall -y pyarrow\")\n\n# 2. Install compatible PyArrow (Fixes Kaggle binary conflict)\nos.system(\"pip install -q -U \\\"pyarrow>=15.0.0\\\"\")\n\n# 3. Install Stack (Pinned TRL 0.12.0 for stability)\nos.system(\"pip install -q -U bitsandbytes transformers peft accelerate\")\nos.system(\"pip install -q -U datasets\") \nos.system(\"pip install -q -U trl==0.12.0\")  # Strictly pinned\nos.system(\"pip install -q -U sentence-transformers faiss-cpu requests\")\n\nprint(\"‚úÖ Installation Complete. Importing libraries...\")\n\n# --- 2. IMPORTS & SETUP ---\nimport torch\nimport pandas as pd\nimport bitsandbytes as bnb\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\nfrom trl import SFTTrainer, SFTConfig\nfrom sentence_transformers import SentenceTransformer\nimport faiss\n\n# --- 2.1 HF LOGIN ---\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login\n    print(\"üîë Logging in to Hugging Face...\")\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n    login(token=hf_token)\n    print(\"‚úÖ Logged in successfully.\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è HF Login Skipped: {e} (Continuing anyway...)\")\n\nwarnings.filterwarnings(\"ignore\")\n\n# Define Paths\nPROJECT_ROOT = \"/kaggle/working/CultureVerse_Final\"\nMODEL_DIR = f\"{PROJECT_ROOT}/model_adapter\"\nVECTOR_DIR = f\"{PROJECT_ROOT}/vector_db\"\nDEPLOYMENT_DIR = f\"{PROJECT_ROOT}/deployment\"\n\nfor d in [PROJECT_ROOT, MODEL_DIR, VECTOR_DIR, DEPLOYMENT_DIR]: \n    os.makedirs(d, exist_ok=True)\n\n# Find Dataset Automatically\nDATA_FILE = None\nfor root, dirs, files in os.walk(\"/kaggle/input\"):\n    for file in files:\n        if file.endswith(\".csv\") and \"cultureverse\" in file.lower():\n            DATA_FILE = os.path.join(root, file)\n            print(f\"üìç Found Data: {DATA_FILE}\")\n            break\n    if DATA_FILE: break\n\nif not DATA_FILE: raise FileNotFoundError(\"‚ùå CSV Not Found! Add data to input.\")\n\n# =================================================================================\n# üü¢ PHASE 1: FINE-TUNING (ROBUST 1 EPOCH CONFIG)\n# =================================================================================\nprint(f\"\\n{'='*40}\")\nprint(\"üß† PHASE 1: TRAINING (Robust 1 Epoch -> ~781 Steps)\")\nprint(f\"{'='*40}\")\n\n# 1. Strict Output Formatter\ndef format_prompt(example):\n    \"\"\"\n    Forces the model to output strictly in the requested 6-heading format.\n    \"\"\"\n    def generate_text(p, l, c, cmp, t, m, ex, tn):\n        has_example = isinstance(ex, str) and len(str(ex).strip()) > 5\n        \n        # System instruction: Defines the persona and the REQUIRED format\n        sys_msg = (\n            \"You are a cultural expert. Analyze the input phrase and provide a detailed structured response \"\n            \"using EXACTLY these headings:\\n\"\n            \"Meaning, Cultural Origin, Usage Context, Emotional Tone, Example, Cultural Tag.\"\n        )\n        \n        if not has_example:\n            sys_msg += \" If no example is provided, GENERATE a realistic, culturally appropriate scenario.\"\n        \n        # Mapping CSV columns to your specific headings\n        # Phrase -> Phrase\n        # Meaning -> Meaning\n        # Language -> Cultural Origin\n        # Category/Complexity -> Usage Context\n        # Tone -> Emotional Tone\n        # Example -> Example\n        # Tags -> Cultural Tag\n        \n        return (f\"<|im_start|>system\\n{sys_msg}<|im_end|>\\n\"\n                f\"<|im_start|>user\\n\"\n                f\"Phrase: '{p}'\\n\"\n                f\"<|im_end|>\\n\"\n                f\"<|im_start|>assistant\\n\"\n                f\"**Meaning:** {m}\\n\"\n                f\"**Cultural Origin:** {l}\\n\"\n                f\"**Usage Context:** {c} ({cmp})\\n\"\n                f\"**Emotional Tone:** {tn}\\n\"\n                f\"**Example:** {ex if has_example else '[GENERATE SCENARIO]'}\\n\"\n                f\"**Cultural Tag:** {t}<|im_end|>\\n\")\n\n    if isinstance(example['phrase'], list):\n        return {\"text\": [generate_text(\n            example['phrase'][i], example['language'][i], example['category'][i],\n            example['complexity'][i], example['tags'][i], example['meaning'][i],\n            example['example'][i], example['tone'][i]\n        ) for i in range(len(example['phrase']))]}\n    \n    return {\"text\": generate_text(\n        example['phrase'], example['language'], example['category'],\n        example['complexity'], example['tags'], example['meaning'],\n        example['example'], example['tone']\n    )}\n\n# 2. Load Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\", trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# 3. Load Model (Dual GPU Optimized)\nprint(\"ü§ñ Loading Qwen 2.5 on Dual GPUs...\")\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2.5-7B-Instruct\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",  # Splits model across T4 x 2\n    trust_remote_code=True,\n    torch_dtype=torch.float16\n)\nmodel.config.use_cache = False\nmodel = prepare_model_for_kbit_training(model)\n\npeft_config = LoraConfig(\n    r=64, lora_alpha=16, lora_dropout=0.05, bias=\"none\", \n    task_type=\"CAUSAL_LM\", \n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n)\nmodel = get_peft_model(model, peft_config)\n\n# 4. Training Arguments (ROBUST 1 EPOCH CONFIG)\ntraining_args = SFTConfig(\n    output_dir=MODEL_DIR,\n    num_train_epochs=1,                 # Exactly 1 Epoch\n    per_device_train_batch_size=4,      # 4 per GPU * 2 GPUs = 8\n    gradient_accumulation_steps=4,      # 8 * 4 = 32 Effective Batch (vs 64 before). \n                                        # This doubles the steps to ~781, ensuring deep learning.\n    learning_rate=2e-4,                 # Robust LR for 700+ steps\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=True,\n    logging_steps=25,\n    optim=\"paged_adamw_8bit\",\n    report_to=\"none\",\n    save_strategy=\"steps\",\n    save_steps=200,\n    save_total_limit=2,\n    group_by_length=True,\n    max_seq_length=512,\n    dataset_text_field=\"text\",\n    packing=False\n)\n\n# 5. Train\ndataset = load_dataset(\"csv\", data_files=DATA_FILE, split=\"train\")\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset.map(format_prompt, batched=True),\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    args=training_args\n)\n\nprint(f\"üöÄ Starting Training (~{len(dataset)//32} Steps | Robust 1 Epoch)...\")\ntrainer.train()\n\nprint(\"üíæ Saving Model...\")\ntrainer.model.save_pretrained(MODEL_DIR)\ntokenizer.save_pretrained(MODEL_DIR)\n\n# Cleanup\ndel model, trainer\ntorch.cuda.empty_cache()\ngc.collect()\nprint(\"‚úÖ Phase 1 Complete.\")\n\n# =================================================================================\n# üü° PHASE 2: DEEP VECTOR DATABASE (ALL 8 COLUMNS)\n# =================================================================================\nprint(f\"\\n{'='*40}\")\nprint(\"üìö PHASE 2: BUILDING DEEP VECTOR DB\")\nprint(f\"{'='*40}\")\n\ndf = pd.read_csv(DATA_FILE).fillna(\"\")\n\nrich_texts = []\nprint(\"üî® Addressing vectors to ALL words in ALL 8 columns...\")\nfor _, r in df.iterrows():\n    text = (f\"Phrase: {r['phrase']} | Language: {r['language']} | \"\n            f\"Meaning: {r['meaning']} | Situation: {r['example']} | \"\n            f\"Tags: {r['tags']} | Category: {r['category']} | \"\n            f\"Tone: {r['tone']} | Complexity: {r['complexity']}\")\n    rich_texts.append(text)\n\nembedder = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\nembeddings = embedder.encode(rich_texts, convert_to_numpy=True, show_progress_bar=True, batch_size=64)\n\nindex = faiss.IndexFlatL2(embeddings.shape[1])\nindex.add(embeddings)\n\nfaiss.write_index(index, f\"{VECTOR_DIR}/idioms.index\")\nwith open(f\"{VECTOR_DIR}/metadata.pkl\", \"wb\") as f:\n    pickle.dump(df.to_dict('records'), f)\n\nprint(\"‚úÖ Phase 2 Complete.\")\n\n# =================================================================================\n# üîµ PHASE 3: HYBRID SYSTEM CREATION\n# =================================================================================\nprint(f\"\\n{'='*40}\")\nprint(\"‚öôÔ∏è PHASE 3: CREATING HYBRID SYSTEM\")\nprint(f\"{'='*40}\")\n\nhybrid_system_code = '''\"\"\"\nHybrid Idiom Explainer - Production System\n\"\"\"\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nimport pickle\nimport requests\nfrom typing import Dict\nfrom functools import lru_cache\n\nclass HybridIdiomExplainer:\n    def __init__(self, model_dir: str, vector_db_dir: str, use_gpu: bool = True):\n        self.device = \"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\"\n        print(f\"Loading system on {self.device}...\")\n        \n        # Tier 2: Vector DB\n        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n        self.index = faiss.read_index(f\"{vector_db_dir}/idioms.index\")\n        with open(f\"{vector_db_dir}/metadata.pkl\", 'rb') as f:\n            self.metadata = pickle.load(f)\n            \n        # Tier 1: Fine-Tuned Model\n        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n        base_model = AutoModelForCausalLM.from_pretrained(\n            \"Qwen/Qwen2.5-7B-Instruct\",\n            torch_dtype=torch.float16 if use_gpu else torch.float32,\n            device_map=\"auto\"\n        )\n        self.model = PeftModel.from_pretrained(base_model, model_dir)\n        self.model.eval()\n        print(f\"‚úÖ System ready ({len(self.metadata)} idioms indexed)\")\n    \n    def _tier1_model(self, phrase: str, language: str) -> Dict:\n        \"\"\"Tier 1: Generative Model\"\"\"\n        # Strict Format Prompt for Inference\n        prompt = (f\"<|im_start|>system\\\\n\"\n                  f\"You are a cultural expert. Analyze the phrase and output the result strictly using these headings:\\\\n\"\n                  f\"Meaning, Cultural Origin, Usage Context, Emotional Tone, Example, Cultural Tag.\\\\n\"\n                  f\"<|im_end|>\\\\n\"\n                  f\"<|im_start|>user\\\\n\"\n                  f\"Phrase: '{phrase}'\\\\n\"\n                  f\"<|im_end|>\\\\n<|im_start|>assistant\\\\n\")\n        \n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs, max_new_tokens=400, temperature=0.7, top_p=0.9, do_sample=True\n            )\n        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        if \"<|im_start|>assistant\" in response:\n            response = response.split(\"<|im_start|>assistant\")[1].strip()\n            \n        return {\"response\": response, \"confidence\": 0.95, \"tier\": \"fine-tuned-model\"}\n    \n    def _tier2_vector(self, phrase: str) -> Dict:\n        \"\"\"Tier 2: Vector Search\"\"\"\n        query_embedding = self.embedder.encode([phrase])\n        distances, indices = self.index.search(query_embedding.astype('float32'), 1)\n        best_idx = indices[0][0]\n        if distances[0][0] < 10.0:\n            similar = self.metadata[best_idx]\n            response = (f\"**Meaning:** {similar['meaning']}\\\\n\"\n                        f\"**Cultural Origin:** {similar['language']}\\\\n\"\n                        f\"**Usage Context:** {similar['category']}\\\\n\"\n                        f\"**Emotional Tone:** {similar['tone']}\\\\n\"\n                        f\"**Example:** {similar['example']}\\\\n\"\n                        f\"**Cultural Tag:** {similar['tags']}\")\n            return {\"response\": response, \"confidence\": 0.85, \"tier\": \"vector-similarity\"}\n        return {\"response\": \"\", \"confidence\": 0.0, \"tier\": \"vector-failed\"}\n    \n    def explain(self, phrase: str, language: str=\"English\") -> Dict:\n        print(f\"\\\\nüîç Analyzing: '{phrase}'\")\n        # 1. Try Model (For creativity & strict format)\n        t1 = self._tier1_model(phrase, language)\n        if t1['confidence'] > 0.8: return t1\n        \n        # 2. Try Vector (For new idioms that might match existing ones semantically)\n        t2 = self._tier2_vector(phrase)\n        if t2['confidence'] > 0.8: return t2\n        \n        return {\"response\": \"Not Found\", \"confidence\": 0.0}\n'''\nwith open(f\"{DEPLOYMENT_DIR}/hybrid_system.py\", \"w\") as f:\n    f.write(hybrid_system_code)\n\nprint(\"‚úÖ hybrid_system.py saved.\")\n\n# =================================================================================\n# üì¶ PHASE 4: PACKAGING\n# =================================================================================\nprint(f\"\\n{'='*40}\")\nprint(\"üì¶ PHASE 4: PACKAGING\")\nprint(f\"{'='*40}\")\n\nreadme = \"\"\"# CultureVerse Hybrid System\n## Usage\n```python\nfrom deployment.hybrid_system import HybridIdiomExplainer\nengine = HybridIdiomExplainer(model_dir=\"../model_adapter\", vector_db_dir=\"../vector_db\")\nresult = engine.explain(\"Spill the beans\", \"English\")\nprint(result['response'])\n\"\"\"\nwith open(f\"{DEPLOYMENT_DIR}/README.md\", \"w\") as f: \n    f.write(readme)\n\nprint(\"üì¶ Zipping Final Package...\") \nshutil.make_archive(\"/kaggle/working/CultureVerse_Final\", 'zip', PROJECT_ROOT)\n\nprint(f\"\\nüéâ DONE! Output saved to: /kaggle/working/CultureVerse_Final.zip\") \nprint(\"üëâ Check the 'Output' tab to download the file.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-25T09:44:38.767129Z","iopub.execute_input":"2025-12-25T09:44:38.767468Z","iopub.status.idle":"2025-12-25T13:23:12.970250Z","shell.execute_reply.started":"2025-12-25T09:44:38.767436Z","shell.execute_reply":"2025-12-25T13:23:12.969067Z"}},"outputs":[{"name":"stdout","text":"üõ†Ô∏è CLEANING ENVIRONMENT & INSTALLING STACK...\nFound existing installation: pyarrow 22.0.0\nUninstalling pyarrow-22.0.0:\n  Successfully uninstalled pyarrow-22.0.0\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 47.7/47.7 MB 43.9 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44.0/44.0 kB 1.3 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 59.1/59.1 MB 33.0 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 12.0/12.0 MB 117.4 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 556.4/556.4 kB 22.7 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 380.9/380.9 kB 25.1 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 512.3/512.3 kB 8.8 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 310.2/310.2 kB 5.9 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 493.7/493.7 kB 861.3 kB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 23.8/23.8 MB 99.5 MB/s eta 0:00:00\n‚úÖ Installation Complete. Importing libraries...\n","output_type":"stream"},{"name":"stderr","text":"2025-12-25 09:45:49.820333: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766655950.206021      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766655950.315116      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766655951.300355      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766655951.300397      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766655951.300400      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766655951.300402      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"üîë Logging in to Hugging Face...\n‚úÖ Logged in successfully.\nüìç Found Data: /kaggle/input/cultureverse-25k-gold-csv/cultureverse_25k_gold.csv\n\n========================================\nüß† PHASE 1: TRAINING (Robust 1 Epoch -> ~781 Steps)\n========================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cc262e687de4ca3be99d1506b66ca05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f2d4663f1944101b3a032e34229723c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fa3d25513a941e5855172013731ab91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fecdf5fd446d46e39d572823c7b12f42"}},"metadata":{}},{"name":"stdout","text":"ü§ñ Loading Qwen 2.5 on Dual GPUs...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"390fd747695a409db79d152eebdac66d"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dae392557c734bb29de310dbb6944958"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26113d92465b4e1bb19dd40923bc8b08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e05dfcd3f630499d8a4d6617b364e883"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf40b6ae8bb74d58895cee7f060ef4bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18f62004a31d4d2caac67f4a20165c3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"583a4759f94c4518a596bbfb293ba0b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88cce61ac2b148aeb4cd2b5c78636fa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec42e282aaef4f969670aecd9c0ca0b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37e6c949921f4e719d161ace7c7077e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f1a15439b9c4748929191925877e17f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf0e099a34ad4bd094f9b2b7494bc7f7"}},"metadata":{}},{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.\n","output_type":"stream"},{"name":"stdout","text":"üöÄ Starting Training (~781 Steps | Robust 1 Epoch)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1563/1563 3:31:28, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>2.873300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.444200</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.445100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.245600</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.385500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.207900</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.317800</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.172400</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.264800</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.152100</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.220000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.140800</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.202700</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.133300</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.184500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.125800</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.164000</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.121400</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>0.156600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.119100</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>0.144600</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.118600</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>0.141200</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.115800</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>0.135300</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.114200</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>0.126000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.115900</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>0.121300</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.112800</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>0.118500</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.113100</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>0.115500</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.111500</td>\n    </tr>\n    <tr>\n      <td>875</td>\n      <td>0.115100</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.110600</td>\n    </tr>\n    <tr>\n      <td>925</td>\n      <td>0.113400</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.108700</td>\n    </tr>\n    <tr>\n      <td>975</td>\n      <td>0.109600</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.109300</td>\n    </tr>\n    <tr>\n      <td>1025</td>\n      <td>0.108000</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.107700</td>\n    </tr>\n    <tr>\n      <td>1075</td>\n      <td>0.108200</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.106800</td>\n    </tr>\n    <tr>\n      <td>1125</td>\n      <td>0.104000</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.106800</td>\n    </tr>\n    <tr>\n      <td>1175</td>\n      <td>0.104200</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.107700</td>\n    </tr>\n    <tr>\n      <td>1225</td>\n      <td>0.104100</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.105500</td>\n    </tr>\n    <tr>\n      <td>1275</td>\n      <td>0.100600</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.107300</td>\n    </tr>\n    <tr>\n      <td>1325</td>\n      <td>0.099600</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.104700</td>\n    </tr>\n    <tr>\n      <td>1375</td>\n      <td>0.098200</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.105400</td>\n    </tr>\n    <tr>\n      <td>1425</td>\n      <td>0.097700</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.105400</td>\n    </tr>\n    <tr>\n      <td>1475</td>\n      <td>0.098700</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.105800</td>\n    </tr>\n    <tr>\n      <td>1525</td>\n      <td>0.098900</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.104200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"üíæ Saving Model...\n‚úÖ Phase 1 Complete.\n\n========================================\nüìö PHASE 2: BUILDING DEEP VECTOR DB\n========================================\nüî® Addressing vectors to ALL words in ALL 8 columns...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fb5210cdebc4d9689837f2bd870a69e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f94d6176b6d4be6a2bdecdb0105d886"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f44d15eba034d82b068fea21dd26f77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98339738ec6241bb8a353c4327348b24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcdb693415714776bbc5409f49865a19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17afed8e9e59453fa195012dd63b3b0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7c5b6fb1f1c40adafda7ed7d1f542ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9aff5c454614fc881496593a609e453"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94ec0b4d73e84be0942e7afe84c63704"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98af8dec51504981b42cc9b5b2985ba4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69048537d69c43ceb0a6f99291fff338"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/391 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"001e11a4d9194529b7aaf1979e79f73b"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Phase 2 Complete.\n\n========================================\n‚öôÔ∏è PHASE 3: CREATING HYBRID SYSTEM\n========================================\n‚úÖ hybrid_system.py saved.\n\n========================================\nüì¶ PHASE 4: PACKAGING\n========================================\nüì¶ Zipping Final Package...\n\nüéâ DONE! Output saved to: /kaggle/working/CultureVerse_Final.zip\nüëâ Check the 'Output' tab to download the file.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nfrom IPython.display import FileLink\n\n# This generates a clickable download link\nprint(\"üëá Click the link below to download your file:\")\nFileLink(r'CultureVerse_Final.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T13:26:37.985213Z","iopub.execute_input":"2025-12-25T13:26:37.986041Z","iopub.status.idle":"2025-12-25T13:26:37.992119Z","shell.execute_reply.started":"2025-12-25T13:26:37.985995Z","shell.execute_reply":"2025-12-25T13:26:37.991590Z"}},"outputs":[{"name":"stdout","text":"üëá Click the link below to download your file:\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/CultureVerse_Final.zip","text/html":"<a href='CultureVerse_Final.zip' target='_blank'>CultureVerse_Final.zip</a><br>"},"metadata":{}}],"execution_count":2}]}